

Set up deps: 
python3 -m pip install -r requirements.txt


Download and install from https://ollama.com/download (DMG). 
Launch the Ollama app once so it starts the local service.

ollama pull llama3

export OLLAMA_MODEL=llama3

Run backend: 
./scripts/run_backend.sh 
(uses uvicorn src.app.main:app --reload --port 8000)

Frontend: 
python3 -m http.server 3000 --directory public

http://localhost:3000

http://localhost:3000; set Backend URL to 
http://localhost:8000

API direct test: 
curl http://127.0.0.1:8000/health and POST to /plan as in README.








########################
steps


python -m pip install -r requirements.txt

run the api:
uvicorn main:app --reload


health check
curl http://127.0.0.1:8000/health

{"status":"ok"}


curl -X POST http://127.0.0.1:8000/plan \
  -H 'Content-Type: application/json' \
  -d '{
    "profile": {"height_cm":175,"weight_kg":70,"age":30,"sex":"male","activity_level":"moderate"},
    "dietary_preference":"omnivore",
    "budget_amount":100,
    "budget_period":"week",
    "meals_per_day":3
  }'







Download and install from https://ollama.com/download (DMG). 
Launch the Ollama app once so it starts the local service.

ollama pull llama3

export OLLAMA_MODEL=llama3


curl -X POST http://127.0.0.1:8000/plan \
-H 'Content-Type: application/json' \
-d '{
"profile": {"height_cm":175,"weight_kg":70,"age":30,"sex":"male","activity_level":"moderate"},
"dietary_preference":"omnivore",
"budget_amount":100,
"budget_period":"week",
"meals_per_day":3,
"use_llm": true
}'





## kill

lsof -i tcp:8000
kill -9 70510