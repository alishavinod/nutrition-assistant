Download and install from https://ollama.com/download (DMG). 
Launch the Ollama app once so it starts the local service.

ollama pull llama3

pip install -r requirements.txt

pip install -U langchain langchain-community langchain-core langsmith langchain-text-splitters
pip list | grep langchain


## for chroma db
export CHROMADB_TELEMETRY=0

export HF_MODEL_ID=meta-llama/Meta-Llama-3.1-8B-Instruct
export OLLAMA_MODEL=llama3

export HF_MODEL_ID=flax-community/t5-recipe-generation
export HF_DEVICE=cuda

Run backend: 
./scripts/run_backend.sh 
(uses uvicorn src.app.main:app --reload --port 8000)

Frontend: 
python3 -m http.server 3000 --directory public
http://localhost:3000


API direct test: 
curl http://127.0.0.1:8000/health and POST to /plan as in README.




## kill processes
kill -9 $(lsof -ti tcp:8000)

